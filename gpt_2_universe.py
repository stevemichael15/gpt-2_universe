# -*- coding: utf-8 -*-
"""GPT-2_universe

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-3JUIXufv18HGLnNWNIoYvBKNFgayku
"""

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("text", data_files = {"train": "/content/universe_dataset.txt"})
dataset

from transformers import AutoTokenizer, AutoModelForCausalLM

model = "gpt2"

tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(model)

tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

def tokenize_function(examples):
    outputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=128,
    )
    outputs["labels"] = outputs["input_ids"].copy()
    return outputs

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    num_proc=2,
    remove_columns=["text"]
)

tokenized_dataset

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir = "./gpt2-universe",
    learning_rate = 2e-5,
    per_device_train_batch_size = 2,
    num_train_epochs = 3,
    weight_decay = 0.01,
    save_total_limit = 2,
    logging_steps = 50,
    fp16 = True
)

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_dataset["train"]
)
trainer.train()

trainer.save_model("./gpt2-universe-finetuned")
tokenizer.save_pretrained("./gpt2-universe-finetuned")

from transformers import pipeline

generator = pipeline("text-generation", model="./gpt2-universe-finetuned", tokenizer=tokenizer)

prompt = "Milky way galaxy"
result = generator(prompt, max_length=100, num_return_sequences=1)

from IPython.display import Markdown
display(Markdown(result[0]["generated_text"]))