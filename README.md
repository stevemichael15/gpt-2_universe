# Fine-Tuned GPT-2 on Universe Knowledge

This repository contains a fine-tuned version of **GPT-2** trained on a custom dataset containing detailed information about the **universe, galaxies, and cosmic phenomena**.  
The goal of this model is to generate text related to space science and astronomy in a natural, coherent way.

---

## Model Overview
- **Base Model:** GPT-2 (small)
- **Framework:** Hugging Face Transformers
- **Dataset:** Custom text corpus (~3000 lines) on universe and astrophysics
- **Task:** Causal language modeling (text generation)
- **Objective:** To create a model that writes space-related content and explanations naturally.
